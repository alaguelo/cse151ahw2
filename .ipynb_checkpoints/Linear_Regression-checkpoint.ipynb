{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Linear regression generally have the form of $Y_{i} = \\theta_{0} + \\theta_{1} x_{1} + \\theta_{2} x_{2} + ...$ <br>\n",
    "There are several ways to find the coefficients of the regression: <br>\n",
    "1. Linear Algebra: $\\hat{\\theta} = (X^{T}X)^{-1}X^{T}Y$ (When X is invertible) <br>\n",
    "2. Gradient Descent: In this case, we need to write out the loss function and try to minimize the loss. <br>\n",
    "$\\hspace{30mm}$ $F(x)$ = Loss Function = MSE = $ \\frac{1}{n}\\sum^{n}_{i=1} (Y_{i} - \\hat{Y_{i}})^{2}$ <br>\n",
    "\n",
    "In this part of the assignment, we will be using the second way to implement this linear regression model. More details about the model's implementation can be found in corresponding lectures.\n",
    "\n",
    "### <font color='red'>ATTENTION: THERE ARE A TOTAL OF 4 QUESTIONS THAT NEED YOUR ANSWERS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages\n",
    "\n",
    "You'll be implement your model in `LinearRegression.py` which should be put under the same directory as the location of `Linear_Regression.ipynb`. Since we have enabled `autoreload`, you only need to import these packages once. You don't need to restart the kernel of this notebook nor rerun the next cell even if you change your implementation for `LinearRegression.py` in the meantime.\n",
    "\n",
    "A suggestion for better productivity if you never used jupyter notebook + python script together: you can split your screen into left and right parts, and have your left part displaying this notebook and have your right part displaying your `LinearRegression.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please do not change this code block\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import numpy, pandas, pyplot for arrays, dataframes, and visualizations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import sklearn model to validate our custom model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Please make sure that your `LinearRegression.py` is under the same folder as this .ipynb notebook\n",
    "from LinearRegression import Linear_Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Perfect Data\n",
    "\n",
    "In this part, we generate a dataset with a perfect linear relationship to test our model's performance. Here, we use the equation: $y = 5x + 10$ to generate our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'x = [ 1  6 11 16 21], y = [ 15  40  65  90 115] for the first 5 values'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([np.arange(1, 1000, 5)]).T\n",
    "y = np.array((5 * X)).flatten() +  10\n",
    "f'x = {X[:5].flatten()}, y = {y[:5]} for the first 5 values'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try to fit our model without any normalization (note: the below cell block could take significant amount of time to complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:2\u001b[0m\n",
      "File \u001b[1;32m~\\Downloads\\LinearRegression.py:60\u001b[0m, in \u001b[0;36mLinear_Regression.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m############### END TODO 1 ###############\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m############### START TODO 2 ###############\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Add bias (if necessary) by concatanating a constant column into X\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Hint: go through HW1 Q5 might be helpful\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept:\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m############### END TODO 2 ###############\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# initialize coefficient\u001b[39;00m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reg = Linear_Regression(num_iter = 10000000)\n",
    "reg.fit(X,y)\n",
    "print(f'\\nNumber of total iterations: {len(reg.loss)} \\nBest Loss: {min(reg.loss)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's try to fit our model with min-max normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "reg_norm = Linear_Regression(num_iter = 10000000, normalize=True)\n",
    "reg_norm.fit(X,y)\n",
    "print(f'\\nNumber of total iterations: {len(reg_norm.loss)} \\nBest Loss: {min(reg_norm.loss)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compare the performance between these two models with/without normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log(reg.loss), label='Linear Regression w/o Normalization')\n",
    "plt.plot(np.log(reg_norm.loss), label='Linear Regression w/ Normalization')\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Loss (Log Scaled)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Question 1: What conclusions can you draw from this experiment? Did normalization help? How and why?**\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: Real-World Data\n",
    "\n",
    "After you complete the first experiment, let's see how our model performs against real-world data.\n",
    "\n",
    "The below dataset is taken from the [Boston Housing dataset](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html), where there are 13 features and 1 target variable.\n",
    "\n",
    "0. CRIM - per capita crime rate by town\n",
    "1. ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "2. INDUS - proportion of non-retail business acres per town.\n",
    "3. CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "4. NOX - nitric oxides concentration (parts per 10 million)\n",
    "5. RM - average number of rooms per dwelling\n",
    "6. AGE - proportion of owner-occupied units built prior to 1940\n",
    "7. DIS - weighted distances to five Boston employment centres\n",
    "8. RAD - index of accessibility to radial highways\n",
    "9. TAX - full-value property-tax rate per \\$10,000\n",
    "10. PTRATIO - pupil-teacher ratio by town\n",
    "11. B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "12. LSTAT - \\% lower status of the population\n",
    "13. MEDV (**TARGET VARIABLE y**) - Median value of owner-occupied homes in $1000's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data'\n",
    "df = pd.read_csv(url, delimiter='\\s+', header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = np.array(df.drop(13, axis=1)), np.array(df[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.min(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the data to fit our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "reg = Linear_Regression(num_iter=100000, normalize=True)\n",
    "reg.fit(X,y)\n",
    "print(f'\\nNumber of total iterations: {len(reg.loss)} \\nBest Loss: {min(reg.loss)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the loss curve of our model on this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.log(reg.loss), label='Linear Regression w/ Normalization')\n",
    "plt.xlabel(\"Number of Iterations\")\n",
    "plt.ylabel(\"Loss (Log Scaled)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify our model, we can compare our model's performance with respect to the linear regression model implemented in scikit-learn (a.k.a. `sklearn`). Scikit-learn is a popular machine learning library in python that provides many classical machine learning algorithms for many different tasks (regression, classification, clustering, etc). It also contains utility functions for preprocessing, calculating metrics, etc.\n",
    "\n",
    "If you implemented your model correctly, you should get a very similar output (difference < 1e-3) for RMSE (Root Mean Squared Error) compared to sklearn linear regressor's RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = df.shape\n",
    "X_norm = X.copy()\n",
    "\n",
    "# TODO: normalize X using the procedure in your model implementation\n",
    "X_norm = \n",
    "\n",
    "# Let's build a model with sklearn\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_norm,y)\n",
    "\n",
    "#Compare Root Mean Squared Error.\n",
    "print(f\"Our Model's RMSE: {(sum((reg.predict(X).flatten() - y)**2)/m)**0.5}\\\n",
    "\\nSklearn Model's RMSE: {(sum((lr.predict(X_norm) - y)**2)/m)**0.5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's have some tweaks with our custom model. First, let's see if an interception (i.e. bias) really helps with our model's performance on the real-world data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%capture\n",
    "reg_bias = Linear_Regression(num_iter=100000, normalize=True, intercept=True)\n",
    "reg_no_bias = Linear_Regression(num_iter=100000, normalize=True, intercept=False)\n",
    "reg_bias.fit(X,y)\n",
    "reg_no_bias.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Our Model's RMSE with Interception: {(sum((reg_bias.predict(X).flatten() - y)**2)/m)**0.5}\\\n",
    "\\nOur Model's RMSE without Interception: {(sum((reg_no_bias.predict(X).flatten() - y)**2)/m)**0.5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>**Question 2: What conclusions can you make here? Does the addition of an intercept make our model perform better?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, let's see if regularization can further help with decreasing our model's loss. Since regularization deals with the problem of overfitting, we need to check our model's performance on the \"unseen\" data. Here, we will split our data into two parts: `training set` and `test set`, where our model will be fit with the training set, and the performance will be evaluated based on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.33, random_state=42)\n",
    "m, n = X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%capture\n",
    "reg = Linear_Regression(num_iter=100000, normalize=True)\n",
    "reg.fit(X_train, y_train)\n",
    "# Feel free to tune the lambda hyperparameter for better performance when penalty (regularization) is applied\n",
    "reg_l1 = Linear_Regression(num_iter=100000, normalize=True, penalty='l1')\n",
    "reg_l1.fit(X_train, y_train)\n",
    "reg_l2 = Linear_Regression(num_iter=100000, normalize=True, penalty='l2')\n",
    "reg_l2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Our Model's RMSE: {(sum((reg.predict(X_test).flatten() - y_test)**2)/m)**0.5}\")\n",
    "print(f\"Our L1 Regularized Model's RMSE: {(sum((reg_l1.predict(X_test).flatten() - y_test)**2)/m)**0.5}\")\n",
    "print(f\"Our L2 Regularized Model's RMSE: {(sum((reg_l2.predict(X_test).flatten() - y_test)**2)/m)**0.5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> **Question 3: What conclusions can you make here? Does the addition of a regularization make our model perform better on the test set? Why does the addition of it make our model perform better/worse?** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see the role of an adaptive learning rate. Let's see our model's performance when adaptive learning rate is disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%capture\n",
    "reg = Linear_Regression(num_iter=100000, normalize=True)\n",
    "reg.fit(X, y)\n",
    "reg_alt = Linear_Regression(num_iter=100000, normalize=True, adaptive=False)\n",
    "reg_alt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Our Model's RMSE with Adaptive LR: {(sum((reg.predict(X).flatten() - y)**2)/m)**0.5}\\\n",
    "\\nOur Model's RMSE without Adaptive LR: {(sum((reg_alt.predict(X).flatten() - y)**2)/m)**0.5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> **Question 4: What conclusions can you make here? Does the addition of an adaptive learning rate make our model perform better? What are your reasonings here?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Answer:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
